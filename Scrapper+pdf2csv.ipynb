{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khush-kantaria/lok-sabha-debates-analysis/blob/main/Scrapper%2Bpdf2csv.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lok Sabha Debates Analysis \n",
        "# Scrapper + pdf2csv\n",
        " "
      ],
      "metadata": {
        "id": "YA9fJioPrE5N"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Required Imports and Downloads\n",
        "\n"
      ],
      "metadata": {
        "id": "1-_ChPfK284h"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0GYAJo7T2tuw",
        "outputId": "e4000440-4c23-4d66-e2fa-6fe83da78517"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z8gLcqHTs0me",
        "outputId": "3aef677b-31b6-4195-d5ff-eeff5a760c85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2022-12-02 11:26:36.430960: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting en-core-web-sm==3.4.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 12.8 MB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /usr/local/lib/python3.8/dist-packages (from en-core-web-sm==3.4.1) (3.4.3)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.23.0)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
            "Requirement already satisfied: typer<0.8.0,>=0.3.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (57.4.0)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
            "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.6)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.1)\n",
            "Requirement already satisfied: pathy>=0.3.5 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.9.0)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /usr/local/lib/python3.8/dist-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.9)\n",
            "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /usr/local/lib/python3.8/dist-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.0 in /usr/local/lib/python3.8/dist-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.8/dist-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.8/dist-packages (from typer<0.8.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (7.1.2)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.8/dist-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tika"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1bQ8Dn6BMKo",
        "outputId": "e0d87761-44cc-4a34-81b5-b066322b6966"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting tika\n",
            "  Downloading tika-1.24.tar.gz (28 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from tika) (57.4.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from tika) (2.23.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->tika) (2022.9.24)\n",
            "Building wheels for collected packages: tika\n",
            "  Building wheel for tika (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for tika: filename=tika-1.24-py3-none-any.whl size=32892 sha256=f74e6a44fefa13a003ff4755f585212a7e6378bc901b1b0108231787de6856fc\n",
            "  Stored in directory: /root/.cache/pip/wheels/75/66/8b/d1acbac7d49f3d98ade76c51ae5d72cec1866131a3b1ad9f82\n",
            "Successfully built tika\n",
            "Installing collected packages: tika\n",
            "Successfully installed tika-1.24\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "import numpy as np\n",
        "import multiprocessing\n",
        "from tika import parser\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "\n",
        "import re\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "Ol675RHQBP6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Preparation"
      ],
      "metadata": {
        "id": "1-_LhOkQri30"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Web Scrapper"
      ],
      "metadata": {
        "id": "8BlnNpFajgOt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# a function to scrape the lok sabha digital library and download the pdfs\n",
        "myvar = 0\n",
        "stred = \"\"\n",
        "def scrape(path):\n",
        "\n",
        "  # extracting folder name and creating a folder in drive\n",
        "  folderName = path[208:212]\n",
        "\n",
        "  if folderName in os.listdir(\"/content/drive/Shareddrives/NLP_Project/Legislative Data/\"):\n",
        "    print(folderName, \" already present\")\n",
        "    \n",
        "  else:\n",
        "    folderPath = \"/content/drive/Shareddrives/NLP_Project/Legislative Data/\" + folderName + \"/pdf/\"\n",
        "    folderPathCSV = \"/content/drive/Shareddrives/NLP_Project/Legislative Data/\" + folderName + \"/csv/\"\n",
        "    os.makedirs(folderPath)\n",
        "    os.makedirs(folderPathCSV)\n",
        "\n",
        "    url = path\n",
        "    links = []\n",
        "\n",
        "    while url != None:\n",
        "      req = requests.get(url)\n",
        "      soup = BeautifulSoup(req.content, 'html.parser')\n",
        "      '''\n",
        "      # panel_heading = soup.findAll('div',attrs={'class':['panel-info','panel-heading']})\n",
        "      # stred = str(panel_heading)\n",
        "      # myvar = \n",
        "      # print(np.array(panel_heading))'''\n",
        "      # trying to fetch all the intermediate urls to the target pdfs\n",
        "      table = soup.find('table', attrs = {'summary':'This table browses all dspace content'})\n",
        "      for row in table.findAll('td', attrs = {'headers':'t4'}):\n",
        "        if(row.a != None):\n",
        "          link = row.a['href']\n",
        "          links.append(\"https://eparlib.nic.in\"+link)\n",
        "      \n",
        "      url = soup.find('a', attrs = {'class' : 'pull-right'})\n",
        "      if url != None:\n",
        "        url = \"https://eparlib.nic.in\" + url['href']\n",
        "    \n",
        "    # get to the pdf from here on\n",
        "    for number,link in enumerate(links):\n",
        "      req = requests.get(link)\n",
        "      soup = BeautifulSoup(req.content, 'html.parser')\n",
        "\n",
        "      table = soup.find('table', attrs = {'class' : 'table panel-body'})\n",
        "      for row in table.findAll('td', attrs = {'align':'center'}):\n",
        "        if(row.a != None):\n",
        "          pdfLink = row.a['href']\n",
        "          pdfLink = \"https://eparlib.nic.in\" + pdfLink\n",
        "\n",
        "          # extracting and saving the pdfs\n",
        "          try:\n",
        "            pdfReq = requests.get(pdfLink)\n",
        "            pdfName = str(number+1)\n",
        "            pdfPath = folderPath + pdfName + \".pdf\"\n",
        "            #uncomment below to save files\n",
        "            with open(pdfPath, 'wb') as f:\n",
        "              f.write(pdfReq.content)\n",
        "\n",
        "          except requests.exceptions.HTTPError as errh:\n",
        "              print(\"Http Error:\",errh)\n",
        "              print(\"Number : \", pdfName)\n",
        "          except requests.exceptions.ConnectionError as errc:\n",
        "              print(\"Error Connecting:\",errc)\n",
        "              print(\"Number : \", pdfName)\n",
        "          except requests.exceptions.Timeout as errt:\n",
        "              print(\"Timeout Error:\",errt)\n",
        "              print(\"Number : \", pdfName)\n",
        "          except requests.exceptions.RequestException as err:\n",
        "              print(\"OOps: Something Else\",err)\n",
        "              print(\"Number : \", pdfName)\n",
        "\n",
        "          \n",
        "\n",
        "    print(\"Done with \", folderName)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    \n",
        "  pool = multiprocessing.Pool()\n",
        "  pool = multiprocessing.Pool(processes=2)\n",
        "  year = input(\"Enter the year: \")\n",
        "\n",
        "\n",
        "  paths = [\n",
        "      \"https://eparlib.nic.in/handle/123456789/7/simple-search?page-token=0431cfac1e20&page-token-value=afefbd25ee4f0d4f006fe90aa5db2d21&location=123456789%2F7&query=&filtername=date&filtertype=contains&filterquery={year}&rpp=100&sort_by=dc.date_dt&order=desc\".format(year=year),\n",
        "  ]\n",
        "  pool.map(scrape,paths)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5BR2lk1eBT7y",
        "outputId": "14f63101-fa57-43ce-f545-482bfd6fe846"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter the year: 2002\n",
            "Done with  2002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PDF 2 CSV converter"
      ],
      "metadata": {
        "id": "rLdY23QPjjwi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def removeend(s):\n",
        "    se={'(',')','.','\\n',' '}\n",
        "    k=len(s)-1\n",
        "    while k!=-1 and (s[k].isupper()==True or s[k] in se):\n",
        "        if s[k]==')':\n",
        "            while k!=-1 and s[k]!='(':\n",
        "                k=k-1\n",
        "        else:\n",
        "            k=k-1\n",
        "    #print(s[i])\n",
        "    return s[:k+1]\n",
        "\n",
        "def removestart(s):\n",
        "    se={' ','-',':','\\n'}\n",
        "    k=0\n",
        "    while k!=len(s) and s[k] in se:\n",
        "        k=k+1\n",
        "    #print(s[i])\n",
        "    return s[k:]\n",
        "\n",
        "def Pdf2CsvFolder(folder_path: str):\n",
        "    pdfs = os.path.join(folder_path, 'pdf')\n",
        "    csvs = os.path.join(folder_path, 'csv')\n",
        "    print(pdfs, csvs)\n",
        "    file_names = os.listdir(pdfs)\n",
        "    for name in tqdm(file_names):\n",
        "        Pdf2Csv(os.path.join(pdfs, name), csvs)\n",
        "\n",
        "def Pdf2Csv(path: str, outDir: str):\n",
        "    if os.path.basename(path)[:-4] in os.listdir(outDir):\n",
        "        # print(\"File\", os.path.basename(path)[:-4], \"already exists\")\n",
        "        return\n",
        "    raw = parser.from_file(path)\n",
        "    content = raw['content'].lower()\n",
        "\n",
        "    i = 0\n",
        "    startindlist = []\n",
        "    for match in re.finditer(r':[^-]', content):\n",
        "        startindlist.append(match.start())\n",
        "        i+=1\n",
        "    \n",
        "    startindlist.append(len(content))\n",
        "\n",
        "    convlist = []\n",
        "    for i in range(len(startindlist)-1):\n",
        "        convlist.append(content[startindlist[i]-20 : startindlist[i+1]])\n",
        "    \n",
        "    for j,i in enumerate(convlist):\n",
        "        convlist[j] = re.sub('\\(Interruptions\\)', '', i)\n",
        "        convlist[j] = re.sub('\\(Interruptions\\)', '', i)\n",
        "    \n",
        "    convlist1 = []\n",
        "\n",
        "    for s in convlist:\n",
        "        s = removeend(s)\n",
        "        s = removestart(s)\n",
        "        convlist1.append(s)\n",
        "    \n",
        "    convlist2=[]\n",
        "    for i in convlist1:\n",
        "        convlist2.append(re.sub(r'\\[*\\(*P?p?laced in L?l?ibrary. See No. L?l? T?t? \\d*\\d\\d\\d\\d*/*\\d\\d\\)*\\]*', '', i))\n",
        "\n",
        "    convlist3=[]\n",
        "    for i in convlist2:\n",
        "        i = re.sub(r'\\d\\d\\*.\\d\\d\\*.20\\d\\d', '', i)\n",
        "        convlist3.append(i)\n",
        "    \n",
        "    convlist4=[]\n",
        "    for i in convlist3:\n",
        "        convlist4.append(re.sub(r'\\d\\d\\.\\d\\d\\s*hrs\\.?', '', i))\n",
        "    \n",
        "    convlist5=[]\n",
        "    for i in convlist4:\n",
        "        i = re.sub(r'\\n', '', i)\n",
        "        i = re.sub(r' +', ' ', i)\n",
        "        i = re.sub(r'_+', ' ', i)\n",
        "        i = re.sub(r'\\.+', ' ', i)\n",
        "        i = re.sub(r'…', ' ', i)\n",
        "        convlist5.append(i)\n",
        "    \n",
        "    for j,i in enumerate(convlist5):\n",
        "        if len(i)!=0 and i[-1]!='.':\n",
        "            convlist5[j] = i + '.'\n",
        "\n",
        "    convlist6 = []\n",
        "    ind =0\n",
        "    for j,i in enumerate(convlist5):\n",
        "        if len(i)!=0:\n",
        "            convlist6.append(convlist5[j])\n",
        "            ind+=1\n",
        "    \n",
        "    convlist6 = convlist6[:-1]\n",
        "\n",
        "    df = pd.DataFrame(convlist6)\n",
        "\n",
        "    df.to_csv(os.path.join(outDir, os.path.basename(path)[:-4]))\n",
        "    # print(\"File\", os.path.basename(path)[:-4], \"done\")"
      ],
      "metadata": {
        "id": "YRSlIVegfYxm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "FolderPath = \"/content/drive/Shareddrives/NLP_Project/Legislative Data/2002/\"\n",
        "\n",
        "Pdf2CsvFolder(FolderPath)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yh_XrJKSlXzR",
        "outputId": "28532760-4869-4b08-82bb-2868f1b10ed5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/Shareddrives/NLP_Project/Legislative Data/2002/pdf /content/drive/Shareddrives/NLP_Project/Legislative Data/2002/csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r  0%|          | 0/85 [00:00<?, ?it/s]2022-12-02 11:49:53,826 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar to /tmp/tika-server.jar.\n",
            "2022-12-02 11:49:54,513 [MainThread  ] [INFO ]  Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "INFO:tika.tika:Retrieving http://search.maven.org/remotecontent?filepath=org/apache/tika/tika-server/1.24/tika-server-1.24.jar.md5 to /tmp/tika-server.jar.md5.\n",
            "2022-12-02 11:49:54,980 [MainThread  ] [WARNI]  Failed to see startup log message; retrying...\n",
            "WARNING:tika.tika:Failed to see startup log message; retrying...\n",
            "100%|██████████| 85/85 [08:07<00:00,  5.73s/it]\n"
          ]
        }
      ]
    }
  ]
}